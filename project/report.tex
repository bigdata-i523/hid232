\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big data and hearing disability}


\author{Rahul Velayutham}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{2661 H 7th St}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{rahuvela@umail.iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{R. Velayutham}


\begin{abstract}
Big Data is rapidly becoming a crucial component in the majority of the fields, be it from medicine to software. Big data technologies help in processing humongous amounts of data in a rapid manner while enabling us to achieve results fast and accurately. Big data is becoming a key player in the restoration of ecological assets like water, forests and the likes. Real time analysis of assets all over the world and the changes are documented and stored how this data can be used and for what purpose is the penultimate question. We dissect the various stages of the rainwater harvesting process and show how the application of big data to each stage can enhance the process.
\end{abstract}

\keywords{Big Data, i523 , HID 232 , Rain Water Harvesting}


\maketitle



\section{Introduction}

Hearing loss, also known as hearing impairment, is a partial or total inability to hear.A deaf person has little to no hearing.Hearing loss may occur in one or both ears.Hearing loss can be temporary or permanent.Hearing loss may be caused by a number of factors, including genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins.A common condition that results in hearing loss is chronic ear infections.Certain infections during pregnancy such as syphilis and rubella may also cause hearing loss.Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear.Hearing loss can be categorized as mild, moderate, moderate-severe, severe, or profound\cite{Wikipedia2017}.

To elaborate on the previous paragraph hearing loss can be categorized into two sections, Congenital Hearing Loss and Acquired Hearing Loss.Under Congenital Hearing Loss two chief factors are Genetic and Prenatal Issues. Under Acquired Hearing Loss the chief factors can be listed as Chronic ear infections( also called Otitis Media), Ototoxic drugs  (medications that can affect aspects of hearing), Diseases that affect hearing (otosclerosis, Ménière's Disease, meningitis, mumps, etc.), Head injury, Perforated eardrum\cite{Academy2017}.

More than 50 percent of the time it is believed that genetic factors cause pediatric hearing loss. Genetic or hereditary hearing loss occurs when a gene from one or both of the parents impacts the development of the intricate process of hearing. Genetic issues can affect any portion of the outer, middle or inner ear, and can cause varying degrees of loss. Options for genetic forms of hearing loss vary widely and can range from hearing aids, medication, surgery, cochlear implants or no treatment at all.Prenatal Issues are non-genetic factors that can potentially cause hearing loss before the birth of the child. Factors such as in utero infection, illnesses, toxins consumed by the mother during pregnancy or cytomegalovirus (CMV)  can be passed on to a child in utero and may cause hearing loss. During the birthing process, procedures performed to save a baby’s life in an emergency, such as a ventilator or a strong antibiotic, can also affect hearing\cite{Wikipedia2017}.

As of 2013 hearing loss affects about 1.1 billion people to some degree\cite{Wikipedia2017}.It causes disability in 5\% (360 to 538 million) and moderate to severe disability in 124 million people \cite{Wikipedia2017} . Of those with moderate to severe disability 108 million live in low and middle-income countries.Of those with hearing loss, it began in 65 million during childhood.Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness.Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture.The term hearing impairment is often viewed negatively as it emphasizes what people cannot do. Despite all of the solutions and rationalizations being made, it cannot be denied however that hearing loss is becoming an important problem in today's society and one whose numbers is constantly increasing\cite{Wikipedia2017}.

Big data is the perhaps the most interesting technological advancement made in the current era, it has roots in almost all fields right from health care to education to even government policies. It is the far reach that makes big data important, it allows users and clients to make better-informed decisions by taking into account almost all factors. doctors are looking towards big data to make more accurate diagnostics and look for new medicines, economists are looking towards big data to make more accurate models. The paper will look into how it can enhance some of the solutions provided for those hard of hearing like hearing aids, closed caption etc. It will also suggest enhancements towards preemptively preventing situations that could lead to hearing loss\cite{Wikipedia2017}.

\section{big data in hearing aids}

\subsection{introduction}


Hearing aids are small electronic devices that you wear in or behind your ear. It makes some sounds louder so that a person with hearing loss can listen, communicate, and participate more fully in daily activities. A hearing aid can help people hear more in both quiet and noisy situations. A hearing aid has three basic parts: a microphone, amplifier, and speaker. The microphone receives sound, which converts it into electrical signals and sends them to an amplifier. The amplifier increases the power of the signals and then sends them to the ear through a speaker.
[image-hearing aid breakdown]
They improve the hearing and speech comprehension of people who have hearing loss that results from damage to the small sensory cells in the inner ear, called hair cells(sensorineural hearing loss). The damage can occur as a result of disease, aging, or injury from noise or certain medicines. A hearing aid magnifies sound vibrations entering the ear. Surviving hair cells detect the larger vibrations and convert them into neural signals that are passed along to the brain. The greater the damage to a person’s hair cells, the more severe the hearing loss, and the greater the hearing aid amplification needed to make up the difference. However, there are practical limits to the amount of amplification a hearing aid can provide. However, if the inner ear is too damaged, a hearing aid would be ineffective.
\newline
There are 3 different styles of hearing aids:
\newline
a.Behind-the-ear (BTE):
Hearing aids consist of a hard plastic case worn behind the ear and connected to a plastic earmold that fits inside the outer ear. The electronic parts are held in the case behind the ear. Sound travels from the hearing aid through the earmold and into the ear. BTE aids are used by people of all ages for mild to profound hearing loss.

b.In-the-ear (ITE):
Hearing aids fit completely inside the outer ear and are used for mild to severe hearing loss. The case holding the electronic components is made of hard plastic. 

c.Canal:
Aids fit into the ear canal and are available in two styles. The in-the-canal (ITC) hearing aid is made to fit the size and shape of a person’s ear canal. A completely-in-canal (CIC) hearing aid is nearly hidden in the ear canal. Both types are used for mild to moderately severe hearing loss.

[figure of different hearing aids]

Hearing aids work differently depending on the electronics used. The two main types of electronics are analog and digital.

Analog aids convert sound waves into electrical signals, which are amplified. The aid is programmed by the manufacturer according to the specifications recommended by your audiologist. An audiologist can program the aid using a computer, and you can change the program for different listening environments—from a small, quiet room to a crowded restaurant etc.

Digital aids convert sound waves into numerical codes, similar to the binary code of a computer, before amplifying them. Because the code also includes information about a sound’s pitch or loudness, the aid can be specially programmed to amplify some frequencies more than others. Digital circuitry gives an audiologist more flexibility in adjusting the aid to a user’s needs and to certain listening environments and can be programmed to focus on sounds coming from a specific direction.

Hearing aids are a fairly popular solution among most age groups and users use them for about 8-9 hours a day \cite{Audiol.2017}.The process of getting a hearing aid is fairly simple. First, you confirm with an ENT / audiologist that you are indeed in need of one. Then a series of audiometery tests are performed to determine the extent of damage / hearing loss incurred. Hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram. Another method for quantifying hearing loss is a speech-in-noise test, which gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss.  A recently developed digit-triple speech-in-noise test may be a more efficient screening test.The audiologist then programs the hearing aid to amplify at an acceptable level.

\subsection{big data in hearing aids}

The working of hearing aids was covered in detail in the previous section now we shall focus on the areas where big data can be applied to help both the doctors and the patients as much as possible. We know that in order to determine the extent of hearing loss an audiometry test will be performed. The test proceeds with a patient being made to sit in a sound proof room and being subjected to listening to a wide variety of sounds ranging from the softest possible sound they can perceive to the loudest possible. The audiologist then charts a graph to figure out the extent of hearing loss. It will look like the below graph.
[figure of audiometery here]
\newline
The problem with this process is it's still random and despite audiologists having great skill and lowering the margin as much as possible they can never be totally accurate nor can they test too much because it is physically demanding on the patient too. Big data can be a great help here. Data collected from multiple patients (with their consent) can be stored making use of technologies like apache pig , hive etc. Then when an initial audiometry analysis has been performed we can use deep learning or simple statistical sampling to obtain a few similar cases via technologies like say apache-spark. From these cases, we can perform a more streamlined audiometry test rather than guesswork and further accurately narrow down the loss coefficients. 
\newline
After the hearing loss estimates are charted down. It is time to program the hearing aid (a hearing aid model is selected by the audiologist in accordance with the hearing loss estimates). The aid is programmed to amplify sound waves in the range where losses are observed and then various simulated environments are performed to determine the level of comfort and extent to which the hearing aid is helping and perform fine-tuning.The problem is the same as previous a very limited range of environment that may / may not be useful to the patient is observed. Using big data once again a more accurate test can be conceived. A user can be presented with the environments patients from the similar range of hearing loss faced and this can be used as a basis for fine-tuning. This process has slowly been making its way into research \cite{peternordquist2017} certainly a few companies \cite{phonak2017}.
\newline
 These days most people make use of digital hearing aids. As previously mentioned digital hearing aids are well equipped to make use of big data in a way they do make use of it albeit in a micro manner. The behavioural patterns of the paitent are recorded like the range of volume increase or decrease in various modes, amount used etc. When the patient visits the audiologist the next time this data is analysed and then corrective changes are made towards the programming of the hearing aid. Big data can play a very big role in proactively doing so. These days most hearing aids have moved on from using a separate remote control towards making use of smartphone apps as a remote. This can be viewed as a huge enabler for big data technologies. Since mobile phones will most of the time be connected to the internet this will enable (with consent) real-time load and store of data using technologies like pig and hadoop of user environments and the current sound wave patterns and amplification used along with other useful data like if the patient is increasing or decreasing volume. Hearing aids are certainly growing smarter in the sense when a mobile communication device is brought near the aid the electromagnetic pulses from the phone is detected by the aid and automatically switches to a phone mode, however for most of the part the user has to switch manually to other modes like theatre, noisy etc. Big data can play a huge role in automatic detection. For starters, big data can be employed to dynamically observe the fluctuations in loudness levels as well observe the fluctuations in background noise to help determine what mode should aid change to. Aise from observing fluctuations it can also compare the current scenario to those who have already encountered such scenarios under similar conditions ( and with a similar hearing loss) rate and then perhaps adjust the volume/setting to a safe appropriate level. note that this would be highly experimental and could also create more problems than it solves. As much as we have powerful machine learning algorithms like deep learning it is impossible for them to predict a solution that best suits a patient after all different patients have different problems and conditions but as it learns more and gains more data (the hallmark of deep learning to learn with more data) there will be a good chance that the algorithms will  provide a solution that really suits the patient.
 \newline
So far we have looked at how big data can make use of sound waves in terms of loudness background noise etc, but there is one more aspect in which big data can help us. Analysing the contents of the speech itself.It has been previously mentioned Big data in language and speech processing is a well-established topic and plenty of papers and discussions exists \cite{Chung2017} \cite{Schuller2015}. Most speech can be well predicted these days and when we take into account that hearing aids these days can make out the direction of which the sound is coming from. Making use of this and the ability of deep learning to possibly predicts parts of speech we can leverage this to accordingly increase or decrease volume. certain words will have pronunciations that are hard to understand or have lower tones or have a high frequency to be repeated. we can take advantage of this. Also, the hearing aid can be programmed to automatically increase the volume when it detects a repetition of sentences/words either due to the patient asking the opposite person to repeat his/her sentence. this can be achieved either by listening to keywords like what, sorry, repeat yourself, etc or by analysing the sound waves and learning from that if the same pattern is being repeated. 

\subsection{Data processing and Technologies}
 
Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data. Pattern recognition systems are in many cases trained from labelled "training" data (supervised learning), but when no labelled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).In machine learning, pattern recognition is the assignment of a label to a given input value.An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.
\newline
From the above explanation, it becomes clear in the manner in which we can apply pattern matching for hearing aids. We could use the binary stream as a basis for calculation and from this stream try to match it to existing patterns and predict the future patterns. If any of the generated patterns are found to have speech that is hard to decipher at that range signals can be sent to the hearing aid to according raise the volume of the hearing aid. Aside from that we can make use of the binary sequences and try to find the best fit pattern match, we can eliminate noise because most hearing aids these days have excellent noise cancelling technology so we can be assured that the sound stream is that of the person who is speaking to the patient. Once we get the best fit we can make a correlation between the pattern identified and the next action to be performed. 
\newline
The discussed methods need not only be limited to merely volume increase and decrease. they can also be applied to the fitting process as we have discussed in detail previously, after obtaining the initial estimate graph we can take the plot points [audiogram picture here] and use it to find the best-fit match of another patient  and fine-tune the hearing aid accordingly saving a lot of time and effort and allowing the entire experience to be pleasant and more productive.
\newline
Apache hadoop, hive etc are just data warehousing software, used for distributed storage and processing of dataset of big data using the MapReduce programming model. It consists of computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.we have previously seen how data is being made available for us the next logical question that comes to mind is how do we store it and using what. The answer to what is somewhat easier to explain. As mentioned previously we can make use of sound waves which gets converted to numerical codes. Mobile phones used to control the hearing aids which have access to the internet can make use of hadoop framework. We can send this data to some API which then uses map reduce to accordingly save it to a location which represents that pattern. while looking for a pattern the analysis made at real time can be used to query the API which will use map reduce to obtain all patterns relevant to the range of hearing loss and then use pattern matching to figure out best fits and suggest feasible solutions. Audiologists can make use of this in the same manner only instead of making use of the API via mobile phones they can do it conventionally by a computer which will allow for stronger processing.

\subsection{Section Summary}

Hearing aids are the most assistive technology a person with hearing impairments could receive which do not require surgery. They are fast becoming an important industry especially with the rising problem of hearing loss. The process of obtaining a hearing aid is simple and straightforward and for a long time it remained static. However with the advent of big data the world of hearing aids has been shaken from its static foundations and is undergoing a paradigm shift in the same manner mobile phones evolved to the current smart model. Right from the process of fitting to using the hearing big data can be applied to almost every stage. the entire process is still in its infancy and there is a huge scope for further development. Pattern matching , deep learning all the concepts are only the tip of the iceberg there are many algorithms and designs that can Be implemented. Thus there is a huge market both towards improving the current crop of hearing aids available and as well as creating jobs.



\section{Big data in closed captioning}

\subsection{Introduction}

The importance of video in today's world cannot be stated enough. in the field of education, more and more classes are being shifted to the online model, professors are moving towards keeping their lecturers in places like udemy, youtube, pluralsight etc because there they are allowed the total freedom to take the course in their own direction without the constraints of time and classroom size. Certain sites like youtube even offer a source of remuneration. Thanks to the advancements in technology like smartphones and cheaper internet the general public is not satisfied with just listening to the audio, they want to watch the video and embrace the whole experience and artists have contributed to it by making their videos so colourful. TV is ever present with people preferring to watch news comfortably and be updated on the go rather than rifle through a newspaper. Even in communication, there is a paradigm shift from normal audio based conversations heading towards video calls, facetime, Whatsapp video chat are just a few of the most popular options available.
\newline
Now that an accord has been reached on how popular the video format is becoming, its time to talk about how difficult it's becoming for people who have hearing disabilities to cope with this changing world. These are people who are struggling to understand communication face to face and now are tasked with trying to understand something available on a digital platform and something they may not even have the power to ask for a repeat in case they misheard. the problem with digital media is mainly the distortion that comes with it. If it is too loud it becomes illegible to understand if it's too soft it is not loud enough to understand and the middle ground isn't much of a help more often than not. Aside from videos hearing impaired patients also find it difficult to perform most of their daily life activities like attending a class etc.
\newline
So what solution do we have that is capable of solving most of the above problems. It can be observed that the community has gone about finding different ways to solve their problems. Sign language translators, lip reading etc. They are all convinient methods of getting by but the problem with them is as good as they are they are way too situational. A more reliable method would be that of closed captions. Note that the art of captions is one that already exists and is made mandatory by governments to make such resources available on request and in general people are very receptive towards these technologies and often go out of their way to enable them.
For students perhaps the most common use of captions is the CART(Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying and the students view this in real time there will always be a delay of a few microseconds but it is still extremely useful. In the case of tv, most channels already have implemented a subtitle system. Pre-recorded shows already have subtitles generated in advance and they are displayed. In case of live settings, the same text from the teleprompter is used as a subtitle or a cart like a system is used where a person types in the text being spoken and it is displayed a few seconds later.In the case of online videos, most content providers have provided the option of loading the subtitle file. The problem now lies in generating/obtaining the subtitle files. Good Samaritans always exist and for important videos more often than not the uploader or someone else will generate their own subtitles. The society is slowly recognising the need for subtitles and in general, you will find the subtitles of the files you are looking for if you look hard enough. There also exist professional agencies who will create subtitles either for free or for a price, though in general, the free ones are already hard at work. 
\newline
The issue at hand today is generating good quality subtitles and fast. The traditional method of generating subtitles by having a person listen and then type the subtitles is fine but it is too slow and cumbersome. It would be faster to have a computer generate the subtitles and have a human change/modify errors. Youtube has its own automatic subtitle generation, Facebook too is developing a similar feature on the same lines. Watson IBM have developed an API that leverages the power of Watsons AI to generate subtitles.

\subsubsection{Manual caption generation}
Manual caption generation as previously stated is a very tedious job it involves a person listening to the audio and then accordingly typing out the captions and storing them for later use. Big data can play a significant role in making thing easier for both the generator and the consumer. First lets deal with the generator. In terms of manual caption generation, there is not much that can be done to aid the person unless he switches to an automatic caption generation system. A few simple steps, however, can make his/her lie much simpler. Using big data the video could be analysed for segments which have very common phrases / repeated segments and then these segments could be stored using map reduce. the next could be using map reduce find if there were translations available previously and generate the captions or store it till the user enters the caption. There are multiple benefits of such a method, perhaps the most important benefit would be it serves as an error detection mechanism improving the subtitle accuracy. Another benefit is it serves as a great training tool for supervised and semi-supervised learning algorithms. For the consumers, the biggest problem is finding subtitles. Using Apache Hadoop etc we can create an API that acts as a centralized database. However, this is an initiative that should be encouraged by the government. This is a problem that affects all citizens and having the government take care of it adds responsibility and accountability.

\subsubsection{Automatic caption generation}
Automatic caption generation has made huge strides in the recent years thanks to the development in the field of AI, semantic web, statistical models etc. In the case of automatic caption generation, the process is amazingly straightforward. First, you select the video for which you want to generate. then you upload/send the data to the respective API. The API then generates the captions you need which you can then embed into the video or load into the video.The favourite method seems to be that of youtube API which gives a very high accuracy rating of around 94\% [cite TheDeafCaptioner2017].  Aside from youtube, there exists IBM Watson which boats of one of the most powerful deep learning features. It should be noted that the youtube method is not exactly real-time sure it can work in real time with a delay of a few seconds but it requires the video to be clipped and sent at regular intervals in order for it to be real time to return a caption response with a slight delay. That being said it doesn't mean that youtube cannot do it in real time its a feature that has not been made available to the public yet, to clarify the speech to text conversion happens at a real-time rate it is just the communication between sender and server that will take some time. Youtube API much like Watson makes use of powerful and complex deep learning neural networks. While the exact process has not been made to the public quite understandably so the general concept will be explained along with an additional Hidden Markov model method.
\newline
\subsubsection{Automatic caption generation markov model}
First, we shall explain a simple and relatively straightforward hidden Markov model a simple but powerful AI algorithm.A formal definition from Wikipedia that helps in understanding the concept of Markov models is given as " In simpler Markov models (like a Markov chain), the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in the hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible. Each state has a probability distribution over the possible output tokens. Therefore, the sequence of tokens generated by an HMM gives some information about the sequence of states.The adjective hidden refers to the state sequence through which the model passes, not to the parameters of the model; the model is still referred to as a hidden Markov model even if these parameters are known exactly." [markov model figure]
\newline
The thought process that follows the following explanation is that of a semi-supervised learning. The training data will compose of just words(audio) with their respective speech tag. Also, the training data will comprise a good set of perfectly captioned videos. More than the videos it is the caption files that are important. The individual words file merely provide us with a corpus file of what words to expect. it is the caption files however that allow us to build the Markov model around which the entire caption generation process will be built upon.Now that we have our corpus file we can begin analysis on the caption file to build the model. What we will be doing is using a very simple probabilistic equation, The probability of observing a sequence Y =y(0),y(1)....y(L-1) of length of L is given by, \[ P(Y) = \sum_{X}^{} \frac{P(Y/X)}{P(X)} \] . To explain this in a more understandable manner we are looking for relationships between words. For example, when a person is introducing themselves a common sentence is "Hello my name is xyz " or "hello I am xyz". Another assumption we will have to make at this point at least with respect to the current Markov method is that we will work with more complete words that span at least 3 or more syllables so that the model can detect them better. With that, the previous sentences become "hello name xyz" and "hello xyz", from this reduction we can now get a new relationship, two of them in fact. the word hello leads to the follow up of the word name and this leads to the follow up of xyz whatever and hello leads to xyz. Now this relationship will be remembered. Now we can also make use of this concept but towards sentences. First, we will have to determine via some statistical method what would be the best average length [ time duration] for sentences. Now we can use the previous principle to observe if there exist any relationship between sentences and remember them. Now we get on to processing the data to generate the captions. First, we have to eliminate noise from the audio. Then we need to split it into n appropriate sections [ n = total length of video/time of average sentence calculated previously].  Now using MapReduce we can quickly determine how many segments already have a translation [ ie find if there are any perfect/reasonable matches] and obtain the captions for those. If there are none available then we go to the next stage of getting captions word by word. After converting it to a data format appropriately [ this has been discussed this previously in the hearing aid section, another alternative manner will be discussed shortly]. Now we determine the first word, again using map reduce we can match the word to a list of probable similar sounding words. To obtain the actual word the magic of probability comes into play. remember the Markov model we had discussed earlier, from it we can calculate the probabilities of a word occurring as the first word and accordingly make a very educated guess on which word is the right word. Following that using the Markov model once we can determine what the next word could be after first narrowing down the suspects we can use the model to infer what the next word could be. that is, given the current word what is the probability that the next word will be the selected word. This way we go about generating captions for every word in the sentence. After this is done for all sentences we can generate a caption file for the entire duration of the video. This method is likely to have a lot of errors since the Markov model described here is rather simplistic therefore the onus will have to be on the administrator to go over the captions generated and accordingly make the corrections. This will not only be useful to the people with disabilities but also help improve the accuracy of the system on the whole. Studies have been carried out and more complex implementation of the same idea has been performed with a rather high accuracy rate can be found here [ cite Markov model paper].
\newline
\subsubsection{Automatic caption generation Deep neural network model}
Now the concept of a neural network will be explained. A neural network is based on the on the same idea of how our brains work. A collection of neurons for information processing and to model the world around us.A very brief explanation would be a neuron sums all the inputs and if the resulting value is higher than a specified threshold it fires.[neural net picture].
The above configuration is called a perceptron. It has n inputs and n weights are real numbers and can be positive or negative.The perceptron consists of weights, summation processor and an activation function. the inputs are multiplied by the individual weights and the summation of all of these is passed to an activation function, we will make use of a step activation function which fires 1 if above the threshold a 0 otherwise. We need to train the perceptron now. This essentially means modifying weights after observing the inputs such that the activation function fires correctly.For all inputs, i, W(i) = W(i) + a*(T-A)*P(i), where a is the learning rate, here, W is the weight vector. P is the input vector. T is the correct output that the perceptron should have known and A is the output given by the perceptron.When an entire pass through all of the input training vectors is completed without an error, the perceptron has learnt. A deep neural network is thus a collection of perceptrons or to be more accurate it is a multiple layered architectures which compromises of an input and output layer and in between them multiple hidden layers.[multilayer network image]
From the image we can observe Each input from the input layer is fed up to each node in the hidden layer, and from there to each node on the output layer. We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer.But to prepare this we require a learning calculation which ought to be able to tune not as it were the weights between the output layer and the hidden layer but moreover the weights between the hidden layer and the input layer. Clearly, it becomes obvious that we will need to tune the inputs between the input layer and hidden layer for this we shall make use of a technique called backpropagation which essentially means we carry the error to the next stage of input and then use these errors to modify the input stage of every layer. to be brief we can summarize it as follows We present a training sample to the neural network (initialised with random weights). Compute the output received by calculating activations of each layer and thus calculate the error. Having calculated the error, we readjust the weights (according to the above-mentioned equations) such that the error decreases. We continue the process for all training samples several times until the weights are not changing too much[cite pokarna]. 
\newline
Now that we have a good understanding of how neural networks work we shall look into how ASR happens via neural networks.we shall go about this step by step. The first problem will be converting sound to bits. We have previously seen digital hearing aids automatically convert sound waves into numerical codes and have extended this concept in multiple places. We shall now go into this a little more in detail and provide an insight into how this could happen. Consider the below wave of sound [sound wave] 
This can be represented on a graph as a simple expression of height vs time. The annotations towards height can be amplitude, frequency whatever a person chooses that suits their purpose best. However, note that this is a bit too scattered and not very uniform which is understandable because over digital media the voice can break.Let us attempt to smooth this signal using one of the many transformation algorithms available[ Nyquist theorem for example]. The result will look like this [figure smooth hello].
Recall it was mentioned that we can simply take the graph as a function of height vs time, so to help visualize this consider the [figure sampled graph]. To represent this in a text format it would look like [ 100,-20,30,89,789,-400,.....345] where each value is a measure of height over a designated unit of time. We need to be careful about how we select this unit of time. The idea is that different syllables have a different pitch we want to exploit that. Hence we want to select a unit of time such that each unit possibly corresponds to one letter. The [figure] gives an insight into how the overall model should look. So after we have trained the network to recognise each letter based on value/set of values we proceed with feeding the current input stream and thereby obtaining each letter. We can then pair these letters to form a word. Note that there is a high chance that many letters could be repeated, decisions must be made based on observations if we need to replace repetitions. Once we have obtained these words there is a possibility of spelling errors using an auto-correct program [there are many good algorithms available on the internet] we can then use the Markov model we previously explored to verify its correctness [ if the word generated fits the model observed]. To save some time we could use map-reduce again to find out if similar patterns exist before and then use it accordingly to determine if there were similar instances before.

\subsubsection{Using Image Processing to generate captions}

It is well known how powerful OCR is it can convert text images into a text file by "reading". What if we could apply a similar concept when people are speaking. People who have hearing disabilities especially those who have severe hearing disabilities and cannot always make use of sign language to get by make use of lip reading. Lip reading has existed for a long time and there is no particular skill or technique about it. People familiarize themselves with the movements of the lips with respect to different sounds and then make use of that pattern. We will not go too deep into the specifics of how to process features and use them to recognise these patterns, we have already exhaustively covered neural networks and Markov models in great detail. Depending on how the features were extracted and stored learning can occur. We shall instead focus on the process of obtaining the images for processing the features.Since we are going to be focusing on extracting the facial features we need a device that can focus on the face. Two devices that could be used are the smartphones and perhaps a not so popular option in google glass. Both devices have powerful cameras with which pictures can be taken. Note that captions generated will be with some delay. Converting these devices into IoT devices shouldn't be particularly hard. Using map reduce we can effectively store these images and in a quick manner, to save data however we could probably convert the image into an rgb matrix text file and send. Map reduce plays a very very important role here, rather than sending the same image again and again if a mapping is found to exist already we can make use of it. To extract features OpenCV has a great library that could aid in the feature extraction making use of these features for processing depends on the implementation. Note that big data here if a good bunch of people are observing the same thing, like for example watching a news broadcast, seminar etc. We can use a combination of big data and IOT to determine these similar users and make one person the central hub, he/she will process while the results are shared between all or alternatively each person can send sections at regular intervals to reduce the processing time. For example, if 4 persons are detected person each person can do 10 seconds order depending on implementation this should help keep the process more real time. Note that however, this would be very situational but in situations where transcripts are not made available and the environment is way too noisy for a speech to text caption system to be implemented this comes very handy. While we are on the topic of using these devices as IoT a future possible implementation, especially for smart TVs, would be to do detect such devices and send the subtitles to the device. Too many times if watching from a  distance subtitles from a distance a person am not be able to read them having it sent n the device would mean the person can see the tv and read comfortably especially in the case of google glass.


\section{Conclusion}




\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\end{document}

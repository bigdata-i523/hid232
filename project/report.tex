\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big data and hearing disability}


\author{Rahul Velayutham}
\affiliation{%
  \institution{Indiana University Bloomington}
  \streetaddress{2661 H 7th St}
  \city{Bloomington} 
  \state{Indiana} 
  \postcode{47408}
}
\email{rahuvela@umail.iu.edu}

% The default list of authors is too long for headers}
\renewcommand{\shortauthors}{R. Velayutham}


\begin{abstract}
Big Data is rapidly becoming a crucial component in the majority of the fields, be it from medicine to software. Big data technologies help in processing humongous amounts of data in a rapid manner while enabling us to achieve results fast and accurately. Big data is becoming a key player in the restoration of ecological assets like water, forests and the likes. Real time analysis of assets all over the world and the changes are documented and stored how this data can be used and for what purpose is the penultimate question. We dissect the various stages of the rainwater harvesting process and show how the application of big data to each stage can enhance the process.
\end{abstract}

\keywords{Big Data, i523 , HID 232 , Rain Water Harvesting}


\maketitle



\section{Introduction}

Hearing loss, also known as hearing impairment, is a partial or total inability to hear.A deaf person has little to no hearing.Hearing loss may occur in one or both ears.Hearing loss can be temporary or permanent.Hearing loss may be caused by a number of factors, including genetics, ageing, exposure to noise, some infections, birth complications, trauma to the ear, and certain medications or toxins.A common condition that results in hearing loss is chronic ear infections.Certain infections during pregnancy such as syphilis and rubella may also cause hearing loss.Hearing loss is diagnosed when hearing testing finds that a person is unable to hear 25 decibels in at least one ear.Hearing loss can be categorized as mild, moderate, moderate-severe, severe, or profound\cite{Wikipedia2017}.

To elaborate on the previous paragraph hearing loss can be categorized into two sections, Congenital Hearing Loss and Acquired Hearing Loss.Under Congenital Hearing Loss two chief factors are Genetic and Prenatal Issues. Under Acquired Hearing Loss the chief factors can be listed as Chronic ear infections( also called Otitis Media), Ototoxic drugs  (medications that can affect aspects of hearing), Diseases that affect hearing (otosclerosis, Ménière's Disease, meningitis, mumps, etc.), Head injury, Perforated eardrum\cite{Academy2017}.

More than 50 percent of the time it is believed that genetic factors cause pediatric hearing loss. Genetic or hereditary hearing loss occurs when a gene from one or both of the parents impacts the development of the intricate process of hearing. Genetic issues can affect any portion of the outer, middle or inner ear, and can cause varying degrees of loss. Options for genetic forms of hearing loss vary widely and can range from hearing aids, medication, surgery, cochlear implants or no treatment at all.Prenatal Issues are non-genetic factors that can potentially cause hearing loss before the birth of the child. Factors such as in utero infection, illnesses, toxins consumed by the mother during pregnancy or cytomegalovirus (CMV)  can be passed on to a child in utero and may cause hearing loss. During the birthing process, procedures performed to save a baby’s life in an emergency, such as a ventilator or a strong antibiotic, can also affect hearing.

As of 2013 hearing loss affects about 1.1 billion people to some degree\cite{Wikipedia2017}.It causes disability in 5\% (360 to 538 million) and moderate to severe disability in 124 million people \cite{Wikipedia2017} . Of those with moderate to severe disability 108 million live in low and middle-income countries.Of those with hearing loss, it began in 65 million during childhood.Those who use sign language and are members of Deaf culture see themselves as having a difference rather than an illness.Most members of Deaf culture oppose attempts to cure deafness and some within this community view cochlear implants with concern as they have the potential to eliminate their culture.The term hearing impairment is often viewed negatively as it emphasizes what people cannot do. Despite all of the solutions and rationalizations being made, it cannot be denied however that hearing loss is becoming an important problem in today's society and one whose numbers is constantly increasing.

Big data is the perhaps the most interesting technological advancement made in the current era, it has roots in almost all fields right from health care to education to even government policies. It is the far reach that makes big data important, it allows users and clients to make better-informed decisions by taking into account almost all factors. doctors are looking towards big data to make more accurate diagnostics and look for new medicines, economists are looking towards big data to make more accurate models. The paper will look into how it can enhance some of the solutions provided for those hard of hearing like hearing aids, closed caption etc. It will also suggest enhancements towards preemptively preventing situations that could lead to hearing loss.

\section{big data in hearing aids}

\subsection{introduction}


Hearing aids are small electronic devices that you wear in or behind your ear. It makes some sounds louder so that a person with hearing loss can listen, communicate, and participate more fully in daily activities. A hearing aid can help people hear more in both quiet and noisy situations. A hearing aid has three basic parts: a microphone, amplifier, and speaker. The microphone receives sound, which converts it into electrical signals and sends them to an amplifier. The amplifier increases the power of the signals and then sends them to the ear through a speaker.
[image-hearing aid breakdown]
They improve the hearing and speech comprehension of people who have hearing loss that results from damage to the small sensory cells in the inner ear, called hair cells(sensorineural hearing loss). The damage can occur as a result of disease, aging, or injury from noise or certain medicines. A hearing aid magnifies sound vibrations entering the ear. Surviving hair cells detect the larger vibrations and convert them into neural signals that are passed along to the brain. The greater the damage to a person’s hair cells, the more severe the hearing loss, and the greater the hearing aid amplification needed to make up the difference. However, there are practical limits to the amount of amplification a hearing aid can provide. However, if the inner ear is too damaged, a hearing aid would be ineffective.
\newline
There are 3 different styles of hearing aids:
\newline
a.Behind-the-ear (BTE):
Hearing aids consist of a hard plastic case worn behind the ear and connected to a plastic earmold that fits inside the outer ear. The electronic parts are held in the case behind the ear. Sound travels from the hearing aid through the earmold and into the ear. BTE aids are used by people of all ages for mild to profound hearing loss.

b.In-the-ear (ITE):
Hearing aids fit completely inside the outer ear and are used for mild to severe hearing loss. The case holding the electronic components is made of hard plastic. 

c.Canal:
Aids fit into the ear canal and are available in two styles. The in-the-canal (ITC) hearing aid is made to fit the size and shape of a person’s ear canal. A completely-in-canal (CIC) hearing aid is nearly hidden in the ear canal. Both types are used for mild to moderately severe hearing loss.

[figure of different hearing aids]

Hearing aids work differently depending on the electronics used. The two main types of electronics are analog and digital.

Analog aids convert sound waves into electrical signals, which are amplified. The aid is programmed by the manufacturer according to the specifications recommended by your audiologist. An audiologist can program the aid using a computer, and you can change the program for different listening environments—from a small, quiet room to a crowded restaurant etc.

Digital aids convert sound waves into numerical codes, similar to the binary code of a computer, before amplifying them. Because the code also includes information about a sound’s pitch or loudness, the aid can be specially programmed to amplify some frequencies more than others. Digital circuitry gives an audiologist more flexibility in adjusting the aid to a user’s needs and to certain listening environments and can be programmed to focus on sounds coming from a specific direction.

Hearing aids are a fairly popular solution among most age groups and users use them for about 8-9 hours a day \cite{Audiol.2017}.The process of getting a hearing aid is fairly simple. First, you confirm with an ENT / audiologist that you are indeed in need of one. Then a series of audiometery tests are performed to determine the extent of damage / hearing loss incurred. Hearing sensitivity can be measured for a range of frequencies and plotted on an audiogram. Another method for quantifying hearing loss is a speech-in-noise test, which gives an indication of how well one can understand speech in a noisy environment. A person with a hearing loss will often be less able to understand speech, especially in noisy conditions. This is especially true for people who have a sensorineural loss – which is by far the most common type of hearing loss.  A recently developed digit-triple speech-in-noise test may be a more efficient screening test.The audiologist then programs the hearing aid to amplify at an acceptable level.

\subsection{big data in hearing aids}

The working of hearing aids was covered in detail in the previous section now we shall focus on the areas where big data can be applied to help both the doctors and the patients as much as possible. We know that in order to determine the extent of hearing loss an audiometry test will be performed. The test proceeds with a patient being made to sit in a sound proof room and being subjected to listening to a wide variety of sounds ranging from the softest possible sound they can perceive to the loudest possible. The audiologist then charts a graph to figure out the extent of hearing loss. It will look like the below graph.
[figure of audiometery here]
\newline
The problem with this process is it's still random and despite audiologists having great skill and lowering the margin as much as possible they can never be totally accurate nor can they test too much because it is physically demanding on the patient too. Big data can be a great help here. Data collected from multiple patients (with their consent) can be stored making use of technologies like apache pig , hive etc. Then when an initial audiometry analysis has been performed we can use deep learning or simple statistical sampling to obtain a few similar cases via technologies like say apache-spark. From these cases, we can perform a more streamlined audiometry test rather than guesswork and further accurately narrow down the loss coefficients. 
\newline
After the hearing loss estimates are charted down. It is time to program the hearing aid (a hearing aid model is selected by the audiologist in accordance with the hearing loss estimates). The aid is programmed to amplify sound waves in the range where losses are observed and then various simulated environments are performed to determine the level of comfort and extent to which the hearing aid is helping and perform fine-tuning.The problem is the same as previous a very limited range of environment that may / may not be useful to the patient is observed. Using big data once again a more accurate test can be conceived. A user can be presented with the environments patients from the similar range of hearing loss faced and this can be used as a basis for fine-tuning. This process has slowly been making its way into research \cite{peternordquist2017} certainly a few companies \cite{phonak2017}.
\newline
 These days most people make use of digital hearing aids. As previously mentioned digital hearing aids are well equipped to make use of big data in a way they do make use of it albeit in a micro manner. The behavioural patterns of the paitent are recorded like the range of volume increase or decrease in various modes, amount used etc. When the patient visits the audiologist the next time this data is analysed and then corrective changes are made towards the programming of the hearing aid. Big data can play a very big role in proactively doing so. These days most hearing aids have moved on from using a separate remote control towards making use of smartphone apps as a remote. This can be viewed as a huge enabler for big data technologies. Since mobile phones will most of the time be connected to the internet this will enable (with consent) real-time load and store of data using technologies like pig and hadoop of user environments and the current sound wave patterns and amplification used along with other useful data like if the patient is increasing or decreasing volume. Hearing aids are certainly growing smarter in the sense when a mobile communication device is brought near the aid the electromagnetic pulses from the phone is detected by the aid and automatically switches to a phone mode, however for most of the part the user has to switch manually to other modes like theatre, noisy etc. Big data can play a huge role in automatic detection. For starters, big data can be employed to dynamically observe the fluctuations in loudness levels as well observe the fluctuations in background noise to help determine what mode should aid change to. Aise from observing fluctuations it can also compare the current scenario to those who have already encountered such scenarios under similar conditions ( and with a similar hearing loss) rate and then perhaps adjust the volume/setting to a safe appropriate level. note that this would be highly experimental and could also create more problems than it solves. As much as we have powerful machine learning algorithms like deep learning it is impossible for them to predict a solution that best suits a patient after all different patients have different problems and conditions but as it learns more and gains more data (the hallmark of deep learning to learn with more data) there will be a good chance that the algorithms will  provide a solution that really suits the patient.
 \newline
So far we have looked at how big data can make use of sound waves in terms of loudness background noise etc, but there is one more aspect in which big data can help us. Analysing the contents of the speech itself.It has been previously mentioned Big data in language and speech processing is a well-established topic and plenty of papers and discussions exists \cite{Chung2017} \cite{Schuller2015}. Most speech can be well predicted these days and when we take into account that hearing aids these days can make out the direction of which the sound is coming from. Making use of this and the ability of deep learning to possibly predicts parts of speech we can leverage this to accordingly increase or decrease volume. certain words will have pronunciations that are hard to understand or have lower tones or have a high frequency to be repeated. we can take advantage of this. Also, the hearing aid can be programmed to automatically increase the volume when it detects a repetition of sentences/words either due to the patient asking the opposite person to repeat his/her sentence. this can be achieved either by listening to keywords like what, sorry, repeat yourself, etc or by analysing the sound waves and learning from that if the same pattern is being repeated. 

\subsection{Data processing and Technologies}
 
Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data. Pattern recognition systems are in many cases trained from labelled "training" data (supervised learning), but when no labelled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).In machine learning, pattern recognition is the assignment of a label to a given input value.An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is "spam" or "non-spam"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.
\newline
From the above explanation, it becomes clear in the manner in which we can apply pattern matching for hearing aids. We could use the binary stream as a basis for calculation and from this stream try to match it to existing patterns and predict the future patterns. If any of the generated patterns are found to have speech that is hard to decipher at that range signals can be sent to the hearing aid to according raise the volume of the hearing aid. Aside from that we can make use of the binary sequences and try to find the best fit pattern match, we can eliminate noise because most hearing aids these days have excellent noise cancelling technology so we can be assured that the sound stream is that of the person who is speaking to the patient. Once we get the best fit we can make a correlation between the pattern identified and the next action to be performed. 
\newline
The discussed methods need not only be limited to merely volume increase and decrease. they can also be applied to the fitting process as we have discussed in detail previously, after obtaining the initial estimate graph we can take the plot points [audiogram picture here] and use it to find the best-fit match of another patient  and fine-tune the hearing aid accordingly saving a lot of time and effort and allowing the entire experience to be pleasant and more productive.
\newline
Apache hadoop, hive etc are just data warehousing software, used for distributed storage and processing of dataset of big data using the MapReduce programming model. It consists of computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework.we have previously seen how data is being made available for us the next logical question that comes to mind is how do we store it and using what. The answer to what is somewhat easier to explain. As mentioned previously we can make use of sound waves which gets converted to numerical codes. Mobile phones used to control the hearing aids which have access to the internet can make use of hadoop framework. We can send this data to some API which then uses map reduce to accordingly save it to a location which represents that pattern. while looking for a pattern the analysis made at real time can be used to query the API which will use map reduce to obtain all patterns relevant to the range of hearing loss and then use pattern matching to figure out best fits and suggest feasible solutions. Audiologists can make use of this in the same manner only instead of making use of the API via mobile phones they can do it conventionally by a computer which will allow for stronger processing.

\subsection{Section Summary}

Hearing aids are the most assistive technology a person with hearing impairments could receive which do not require surgery. They are fast becoming an important industry especially with the rising problem of hearing loss. The process of obtaining a hearing aid is simple and straightforward and for a long time it remained static. However with the advent of big data the world of hearing aids has been shaken from its static foundations and is undergoing a paradigm shift in the same manner mobile phones evolved to the current smart model. Right from the process of fitting to using the hearing big data can be applied to almost every stage. the entire process is still in its infancy and there is a huge scope for further development. Pattern matching , deep learning all the concepts are only the tip of the iceberg there are many algorithms and designs that can Be implemented. Thus there is a huge market both towards improving the current crop of hearing aids available and as well as creating jobs.



\section{Big data in closed captioning}

The importance of video in today's world cannot be stated enough. in the field of education, more and more classes are being shifted to the online model, professors are moving towards keeping their lecturers in places like udemy, youtube, pluralsight etc because there they are allowed the total freedom to take the course in their own direction without the constraints of time and classroom size. Certain sites like youtube even offer a source of remuneration. Thanks to the advancements in technology like smartphones and cheaper internet the general public is not satisfied with just listening to the audio, they want to watch the video and embrace the whole experience and artists have contributed to it by making their videos so colourful. TV is ever present with people preferring to watch news comfortably and be updated on the go rather than rifle through a newspaper. Even in communication, there is a paradigm shift from normal audio based conversations heading towards video calls, facetime, Whatsapp video chat are just a few of the most popular options available.
\newline
Now that an accord has been reached on how popular the video format is becoming, its time to talk about how difficult it's becoming for people who have hearing disabilities to cope with this changing world. These are people who are struggling to understand communication face to face and now are tasked with trying to understand something available on a digital platform and something they may not even have the power to ask for a repeat in case they misheard. the problem with digital media is mainly the distortion that comes with it. If it is too loud it becomes illegible to understand if it's too soft it is not loud enough to understand and the middle ground isn't much of a help more often than not. Aside from videos hearing impaired patients also find it difficult to perform most of their daily life activities like attending a class etc.
\newline
So what solution do we have that is capable of solving most of the above problems. It can be observed that the community has gone about finding different ways to solve their problems. Sign language translators, lip reading etc. They are all convinient methods of getting by but the problem with them is as good as they are they are way too situational. A more reliable method would be that of closed captions. Note that the art of captions is one that already exists and is made mandatory by governments to make such resources available on request and in general people are very receptive towards these technologies and often go out of their way to enable them.
For students perhaps the most common use of captions is the CART(Communication Access Real Time) systems, where an individual types a captioning of what the teacher is saying and the students view this in real time there will always be a delay of a few microseconds but it is still extremely useful. In the case of tv, most channels already have implemented a subtitle system. Pre-recorded shows already have subtitles generated in advance and they are displayed. In case of live settings, the same text from the teleprompter is used as a subtitle or a cart like a system is used where a person types in the text being spoken and it is displayed a few seconds later.In the case of online videos, most content providers have provided the option of loading the subtitle file. The problem now lies in generating/obtaining the subtitle files. Good Samaritans always exist and for important videos more often than not the uploader or someone else will generate their own subtitles. The society is slowly recognising the need for subtitles and in general, you will find the subtitles of the files you are looking for if you look hard enough. There also exist professional agencies who will create subtitles either for free or for a price, though in general, the free ones are already hard at work. 
\newline
The issue at hand today is generating good quality subtitles and fast. The traditional method of generating subtitles by having a person listen and then type the subtitles is fine but it is too slow and cumbersome. It would be faster to have a computer generate the subtitles and have a human change/modify errors. Youtube has its own automatic subtitle generation, Facebook too is developing a similar feature on the same lines. Watson IBM have developed an API that leverages the power of Watsons AI to generate subtitles.

\subsection{Big Data in Closed Captioning}


\subsubsection{Manual caption generation}
Manual caption generation as previously stated is a very tedious job it involves a person listening to the audio and then accordingly typing out the captions and storing them for later use. Big data can play a significant role in making thing easier for both the generator and the consumer. First lets deal with the generator. In terms of manual caption generation, there is not much that can be done to aid the person unless he switches to an automatic caption generation system. A few simple steps, however, can make his/her lie much simpler. Using big data the video could be analysed for segments which have very common phrases / repeated segments and then these segments could be stored using map reduce. the next could be using map reduce find if there were translations available previously and generate the captions or store it till the user enters the caption. There are multiple benefits of such a method, perhaps the most important benefit would be it serves as an error detection mechanism improving the subtitle accuracy. Another benefit is it serves as a great training tool for supervised and semi-supervised learning algorithms. For the consumers, the biggest problem is finding subtitles. Using Apache Hadoop etc we can create an API that acts as a centralized database. However, this is an initiative that should be encouraged by the government. This is a problem that affects all citizens and having the government take care of it adds responsibility and accountability.

\subsubsection{Automatic caption generation}



\section{Conclusion}




\begin{acks}

  The authors would like to thank Dr. Gregor von Laszewski for his support and suggestions to write this paper.

\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 


\end{document}
